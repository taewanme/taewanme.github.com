+++
date = "2020-03-15T01:20:25+09:00"
description = "오라클 클라우드의 CLI 툴인 OCI CLI와 JQ를 사용하는 방식을 정리합니다."
title ="예제를 중심으로 정리한 OCI CLI와 jq 레시피"
thumbnailInList = "https://taewanmerepo.github.io/2020/03/ocicli/list.jpg"
thumbnailInPost = "https://taewanmerepo.github.io/2020/ocidocs/cli/020.jpg"
tags = ["oci", "오라클 클라우드", "oci cli", "jq"]
categories = ["Cloud"]
author = "taewan.kim"
disclaimer = "true"
+++

OCI CLI는 OCI(Oracle Cloud Infrastructure) 자원 관리를 돕는 명령어 기반  도구입니다. jq는 json의 반환 값을 처리하는데 매우 편리한 도구로 CLI 형태의 도구와 궁합이 잘 맞는 유틸리티입니다. 

이 문서에서는 OCI CLI 활용에 도움이 될 OCI CLI 명령을 레시피 형태로 정리해 보겠습니다. 이 문서에 레시피는 지속적으로 업데이트 할 예정입니다. 필요한 명령은 댓글로 남겨주시면 반영하겠습니다.  


## 사전 준비 

이 문서는 다음과 같은 사항을 전제로 합니다. 이 문서를 따라하기 형태로 사용하기 위해서는 다음 조건을 만족해야 합니다. 조건을 만족하지 않을 경우 참조 문서를 활용하여 사전 준비하시기 바랍니다. 

|따라하기를 위한 조건|사전 준비를 위한 참조 문서|
|----|----|
|사용할 수 있는 오라클 클라우드(OCI CLI) 계정이 있음|- {{< extlink link="/oci_docs/10_quickstart/how_to_sign_up_oci/" desc="OCI FREE TIER 계정 등록">}}|
|OCI CLI가 설치됨|- {{< extlink link="/oci_docs/80_oci_tools/oci_cli/installation_on_linux_and_mac/" desc="[OCI CLI 설치] Linux & Mac OS">}} <br/> - {{< extlink link="/oci_docs/80_oci_tools/oci_cli/installation_on_window/" desc="[OCI CLI 설치] Window 10">}} |
|OCI CLI 설정 완료|- {{< extlink link="/oci_docs/80_oci_tools/oci_cli/installation_on_linux_and_mac/" desc="[OCI CLI 설치] Linux & Mac OS">}} <br/> - {{< extlink link="/oci_docs/80_oci_tools/oci_cli/config_of_ocicli/" desc="OCI CLI 설정">}} |
|jq 설치 완료|- {{< extlink link="https://www.44bits.io/ko/post/cli_json_processor_jq_basic_syntax#jq-%EC%84%A4%EC%B9%98" desc="jq 설치 가이드">}}|

## 환경 변수 등록

OCI CLI에서를 사용할 때 __COMPARTMENT OCID__와 __OCI 사용자 OCID__를 자주 사용합니다. 두 값을 환경 변수로 등록하면 OCI CLI 사용에 매우 편리합니다. 사용할 __COMPARTMENT OCID__와 __OCI 사용자 OCID__를 다음과 같은 이름으로 등록합니다. 

|값 유형|환경변수명|정보 확인방법|
|----|----|----|
|__OCI 사용자 OCID__|UOCID|{{< extlink link="/oci_docs/98_misc_tips/how_to_collect_basic_info_of_oci/#oci-user%EC%9D%98-ocid" desc="OCI User의 OCID 확인 방법">}}|
|__COMPARTMENT OCID__|COCID|{{< extlink link="/oci_docs/98_misc_tips/how_to_collect_basic_info_of_oci/#compartment-ocid" desc="Compartment OCID 확인 방법">}}|

### 리눅스와 Mac OS X 환경 변수 등록

이번 문서는 Mac OS X와 Linux를 기준으로 소개합니다. 윈도우 에서는 시스템 환경 변수를 등록하는 부분과 환경 변수를 참조하는 방식(__%EVN_VARIABLE%__)만 차이가 납니다. 그 이외는 윈도우와 Mac OS X와 Linux에서 모두 동일하게 적용됩니다.

다음과 같은 명령으로 환경 변수 UOCID와 COCID를 등록합니다.

```
 $ export UOCID=ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna
 $ export COCID=ocid1.compartment.oc1..aaaaaaaar7os3x3p4e6dcak22z7m4zg2ogki6kr34b6sdlkcppzlfij7lvwq
 $ 
```

환경 변수 등록 결과는 다음과 같이 evn 명령으로 확인할 수 있습니다.

```
 $ env|grep OCID
UOCID=ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna
COCID=ocid1.compartment.oc1..aaaaaaaar7os3x3p4e6dcak22z7m4zg2ogki6kr34b6sdlkcppzlfij7lvwq
 $ 
```

두 환경 변수는 앞으로 다음과 같은 __$ENV_VARIABLE__방식으로 참조합니다.

```
 $ echo $UOCID 
ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna
 $ echo $COCID
ocid1.compartment.oc1..aaaaaaaar7os3x3p4e6dcak22z7m4zg2ogki6kr34b6sdlkcppzlfij7lvwq
 $
```

이제 부터 oci 각 서브 명령에 대한 레시피를 정리합니다.

## 1. oci iam

__oci iam__ 명령은 OCI Identity and Access Management Service 자원을 관리하는 명령을 제공합니다.

### 1.1 availability domain : 도메인 목록

현재 사용중인 OCI 리전의 AD(Availability Domain) 목록을 조회합니다.

```
 $ oci iam availability-domain list -c $COCID
{
  "data": [
    {
      "compartment-id": "ocid1.compartment.oc1..aaaaaaaar7os3x3p4e6dcak22z7m4zg2ogki6kr34b6sdlkcppzlfij7lvwq",
      "id": "ocid1.availabilitydomain.oc1..aaaaaaaatrwxaogr7dl4yschqtrmqrdv6uzis3mgbnomiagqrfhcb7mxsfdq",
      "name": "fttO:US-ASHBURN-AD-1"
    },
    {
      "compartment-id": "ocid1.compartment.oc1..aaaaaaaar7os3x3p4e6dcak22z7m4zg2ogki6kr34b6sdlkcppzlfij7lvwq",
      "id": "ocid1.availabilitydomain.oc1..aaaaaaaaztunlny6ae4yw2vghp5go2zceaonwp6wiioe3tnh2vlaxjjl2n3a",
      "name": "fttO:US-ASHBURN-AD-2"
    },
    {
      "compartment-id": "ocid1.compartment.oc1..aaaaaaaar7os3x3p4e6dcak22z7m4zg2ogki6kr34b6sdlkcppzlfij7lvwq",
      "id": "ocid1.availabilitydomain.oc1..aaaaaaaauvt2n7pijol7uqgdnnsoojcukrijtmcltvfwxazmitk235wyohta",
      "name": "fttO:US-ASHBURN-AD-3"
    }
  ]
}
 $
```

### 1.2 API KEY : 도메인 목록

OCI USER에 등록하 api-key 정보를 조회합니다.

```
 $ oci iam user api-key list --user-id $UOCID
{
  "data": [
    {
      "fingerprint": "aa:41:d3:08:3d:f1:61:19:ad:9e:db:bc:2e:de:82:43",
      "inactive-status": null,
      "key-id": "ocid1.tenancy.oc1..aaaaaaaa6ma7kq3bsif76uzqidv22cajs3fpesgpqmmsgxihlbcemkklrsqa/ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna/aa:41:d3:08:3d:f1:61:19:ad:9e:db:bc:2e:de:82:43",
      "key-value": "-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAp7gmJQu1g0wd1UNHht1g\n2NcAwE03Gi10DFHm/VXHOpZe+WH4igKCoCReg5sfX8winihySEgKeJTUrASRMvY8\nZ+WkpgAjmP0RVdM7i89jzBy1HDYTWSQouU+R/sWaTmbSpg18RgJF2Fg0znxJOkTD\nPqPIPxSf2f62m/SLZGz4teuvuUYQ3bXlDjTqio2x0vWXKwWVJqnY3tojNuRHYnbZ\nJGy55VTQGog4B1jaeE20xghDhx3TXHC1DeOS3lrIvxdQ0NnYhqwdcxv6lhwkACCc\nXcOjbdjqBARmkS+n/P+eh4wG1MqnGp7QnqymSp6rZaIU0dmdap2oym7yWMmtouPY\nmQIDAQAB\n-----END PUBLIC KEY-----",
      "lifecycle-state": "ACTIVE",
      "time-created": "2020-03-02T23:45:28.604000+00:00",
      "user-id": "ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna"
    },
    {
      "fingerprint": "24:4a:11:fb:38:b6:8e:49:35:3f:cb:aa:40:52:a3:fc",
      "inactive-status": null,
      "key-id": "ocid1.tenancy.oc1..aaaaaaaa6ma7kq3bsif76uzqidv22cajs3fpesgpqmmsgxihlbcemkklrsqa/ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna/24:4a:11:fb:38:b6:8e:49:35:3f:cb:aa:40:52:a3:fc",
      "key-value": "-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxmhBZEA/9BZncLHlEm5i\nun5mdCnrE2F+qSNsqQjYHH6Gxp9sHvWwPe8P0GpQd/KeS64mdEQ9kJwoi+8swW2H\nRBSX3YCChsokaITdkWYb/jSm3zMXxO6+9dcG1P7lN5STZ8mrNagI5ScXzegEa77l\nAc4neKxoqgZsUgVl5Ta9r5q6GMY7Z02dgcgeg3jgLCXDO0Fx7ekY/FEOr+7deJ/L\nXhexPNqB8sgjS3PxEmSnV0oTSpfsCV25tVXsqaqCm7upOatLwcFkUcCxv1c21mUn\nuR8pO+A0qyXsjs1Qsi2peGtkVdCPap7neUqVv3vN9q5h53gXHcYFM3D9tmyNbdBL\nvwIDAQAB\n-----END PUBLIC KEY-----",
      "lifecycle-state": "ACTIVE",
      "time-created": "2020-03-05T09:33:44.998000+00:00",
      "user-id": "ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna"
    },
    {
      "fingerprint": "2e:a9:fe:8b:15:58:c8:17:41:09:e3:96:95:09:e8:95",
      "inactive-status": null,
      "key-id": "ocid1.tenancy.oc1..aaaaaaaa6ma7kq3bsif76uzqidv22cajs3fpesgpqmmsgxihlbcemkklrsqa/ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna/2e:a9:fe:8b:15:58:c8:17:41:09:e3:96:95:09:e8:95",
      "key-value": "-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAroPSOVXi457EmlJtn3BL\naec0Jw1IGCRIZjTQkac0dPcMmxq1VsQ5um4R0iao9we82tkaHcfGJjzPs/mXAdoX\n2cR6vdtHy80kIoJ9RBNj2nvfiv++7UwP8leMI7H0dOsUi9sAck0yaIj6lMaSUKuB\nV1VRmpeB1hx7Ssz6xuJZYAj7bGYCkWKeltOMrs8STKzTyFFcIw/A5GsI2w3KyJsf\n8+5nFZQjRIrtqNWXwRvPv8dsYM8fYfpduHFhLUvXWxwkig8acTbjOpebcVH4ivTd\nmmrlBtc1RX1fcO+VmbzrKGJJ5y7Mya1MwzvI+B93hhE/a8slO6rTBH1vFt6BSO8L\nmQIDAQAB\n-----END PUBLIC KEY-----",
      "lifecycle-state": "ACTIVE",
      "time-created": "2020-03-05T09:36:37.887000+00:00",
      "user-id": "ocid1.user.oc1..aaaaaaaaqddvfo3p4tprztenweniyg3zebqgqgpbnk4cjspyzcluzi2yndna"
    }
  ]
}
 $
```

이 명령은 OCI Console에서 다음 이미지의 정보를 출력합니다. 

![](https://taewanmerepo.github.io/2020/03/ocicli/010.jpg)


## 2. oci compute 

__oci compute__ 명령은 OCI Compute Service 자원을 관리하는 명령을 제공합니다.

### compute image : 이미지 검색

현재 사용 가능한 compute image 목록을 조회합니다. 


oci compute image list -c $COCID | jq '.data [] | select ( .create-image-allowed == true) |  select ( "operating-system"| contains ( "Linux") ) | "display-name"+ ","+ "id"+ ","+ "operating-system"+ ","+ "operating-system-version" ' 

oci compute image list -c $COCID | jq -r '.data [] | select ( "create-image-allowed"== true) | * ' 

.EnvironmentName == "hello-dev"

hello.json | jq '.[] | select(.EnvironmentName == "hello-dev")'

## 참고 문서

- {{< extlink link="https://docs.cloud.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm" desc="Command Line Interface (CLI)">}}
- {{< extlink link="https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/2.9.6/oci_cli_docs/" desc="Oracle Cloud Infrastructure CLI Command Reference">}}
- {{< extlink link="https://docs.cloud.oracle.com/en-us/iaas/tools/oci-cli/2.9.6/oci_cli_docs/" desc="Oracle Cloud Infrastructure CLI Command Reference">}}

데모 진행을 위하여 다음과 같은 환경이 구성되어 있다고 가정합니다. 다음 자원이 없다면 “자원 생성 참조 문서“를 참조하여 해당 자원을 생성하시기 바랍니다.

|자원 유형|자원 이름|해당 자원을 포함하는 Compartment| 자원 생성 참조 문서|
|----|----|----|----|
|Compartment|sandbox|root|{{< extlink link="/oci_docs/10_quickstart/create_compartment/" desc="Compartment 추가">}} |
|VCN|demo_vcn|root > sandbox|{{< extlink link="/oci_docs/10_quickstart/vcn/simple_vcn/" desc="간편 VCN 만들기">}} |


## 멀티 Compute 인스턴스 공유 볼륨 구성

여러 Compute가 공유하는 Block Volume 구성은 다음과 같은 순서로 구성됩니다. 

1. Compute VM 인스턴스 프로비저닝
1. 포트 공개 설정 
1. 공유 Block Volume 생성
1. Compute에 Block Volume 연결
1. 2개 인스턴스에 OCFS2 설치 및 구성
1. cluster-vm01 공유 볼륨 포맷/마운트
1. cluster-vm01 공유 볼륨 테스트
1. cluster-vm02 공유 볼륨 마운트
1. cluster-vm02 공유 볼륨 테스트

### 1. Compute 인스턴스 프로비저닝

OCFS2를 설치하고 Blokc Volume을 읽기/쓰기 모드로 공유할 Compute 인스턴스 2개를 만듭니다. 다음과 같은 설정으로 Compute 인스턴스 2개를 만드시기 바랍니다. 

|설정 항목|cluster_vm01|cluster_vm02|
|----|----|----|
|Name your instance|cluster-vm01|cluster-vm02|
|Operating System or Image Source|Oracle Linux 7.7|Oracle Linux 7.7|
|Availability Domain|AD 1|AD 1|
|Instance Type|Virtual Machine|Virtual Machine|
|Instance Shape|VM.Stanndard2.1|VM.Stanndard2.1|
|Virtaul cloud network compartment|sandbox|sandbox|
|Virtaul cloud network|demo_vcn|demo_vcn|
|Subnet Compartment|sandbox|sandbox|
|Subnet|Public Subnet-demo_vcn|Public Subnet-demo_vcn|
|Assign a public IP address|체크|체크|
|Choose SSH Key file|체크|체크|
|public key|id_rsa.pub|id_rsa.pub|

Compute 인스턴스 프로비저닝이 어려우실 경우 다음 문서를 참조하시기 바랍니다. 
- {{< extlink link="/oci_docs/10_quickstart/compute/basic_linux_vm/linux_vm_provisioning/" desc="Oracle Linux VM 인스턴스 생성">}} 참조: __2.3 SSH 접속__ 절까지 수행

ssh 통신에 사용되는 SSH Key Pair(Public Key, Private Key)를 준비가 어려우실 경우 다음 문서를 참조하시기 바랍니다. 
- {{< extlink link="/oci_docs/98_misc_tips/ssh_key_pairs/" desc="운영체제별 SSH Key Pair 준비">}} 

cluster_vm01와 cluster_vm02 인스턴스가 만들어지면 Compute 관리 페이지에 다음 이미지와 목록이 출력됩니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/310.jpg)

앞에서 생성한 Compute 인스턴스는 다음과 같은 공개 IP와 비공개 IP를 갖습니다. 이 IP 정보는 아래 설명에서 사용됩니다. 

|인스턴스 이름|공개 IP 주소|비공개 IP 주소|서브넷|
|----|----|----|----|
|cluster-vm01|150.136.62.165|10.0.0.27|Public Subnet-demo_vcn|
|cluster-vm02|150.136.53.151|10.0.0.28|Public Subnet-demo_vcn|


### 2. 포트 공개 설정 

OCFS2를 구성하기 위해서는 2개 포트를 공개해야 합니다. 7777과 3260입니다. 7777 포트는 OCFS2가 클러스터를 구성하기 위해서 사용하는 포트이고 3260은 Volume Gateway 포트입니다. 두 포트를 OCI Security List와 OS 방화벽에서 오픈합니다. 

#### Security List에서 포트 오픈

현재 사용하고 있는 demo_vcn의 Public Subnet-demo_vcn 서브넷에 기본적용되는 __Default Security List for demo_vcn__ Security List에 다음 2개 룰을 추가합니다. 

|포트|Stateless|Source Type|Source CIDR|IP Protocol|Source Port Range|Destination Port Range|
|----|----|----|----|----|----|----|
|7777|체크X|CIDR|10.0.0.0/16|TCP|All|7777|
|3260|체크X|CIDR|10.0.0.0/16|TCP|All|3260|

__Default Security List for demo_vcn__ Security List에 2개 룰을 추가하는 것이 어려우실 경우 다음 문서를 참조하시기 바랍니다. 

- {{< extlink link="/oci_docs/10_quickstart/vcn/simple_vcn/#1-3-security-list에-secury-rule추가" desc="1.3 Security List에 Secury Rule추가 ">}} 

2개 룰이 추가디면 __Default Security List for demo_vcn__의 Ingress Rules은 다음과 같은 형태가 됩니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/020.jpg)

#### 리눅스 방화벽 오픈

cluster_vm01과 cluster_vm02에 ssh 접속한 후 다음 4개 명령을 실행하여 방화벽을 오픈합니다. 

```
sudo firewall-cmd --zone=public --permanent --add-port=7777/tcp
sudo firewall-cmd --zone=public --permanent --add-port=3260/tcp
sudo firewall-cmd --complete-reload
sudo firewall-cmd --info-zone public ##오픈 상태 확인
```

다음 이미지는 cluster_vm01과 cluster_vm02에서 각각 위 4개 명령을 시행한 결과입니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/320.jpg)

### 3. 공유 Block Volume 생성

cluster_vm01과 cluster_vm02에서 ocfs2로 클러스터를 구성하고 공유할 Block Storage를 생성할 차례입니다. Block Volues 관리 페이지에서 Block Volume을 생성합니다. Block Volues 관리 페이지 이동 메뉴 패스는 다음과 같습니다.

- __OCI 콘솔 왼쪽 위 햄버그 버튼__ >> __Block Storage__ >>  __Block Volumes__

![](https://taewanmerepo.github.io/2020/ocidocs/block/010.jpg?classes=border,shadow) 

Block Volume 관리 페이지에서 앞으로 생성할 Block Volume을 할당할 Compartment로 sandbox(root/sandbox)를 선택합니다. 그리고 “Create Block Volume” 버튼을 클릭합니다. 

![](https://taewanmerepo.github.io/2020/ocidocs/block/020.jpg?classes=border,shadow) 

“Create Block Volume” 버튼을 클릭하면 __Create Block Volume" 생성 입력 폼이 나타납니다. 입력 폼에 다음과 같은 입력값을 설정합니다. 

|입력 항목|설정값|설명|
|----|----|----|
|Name|shared_volume||
|Create In Compartment|sandbox|Block Volume을 할당할 Compartment|
|Availability Domain|fttO:US-ASHBURN-AD-1|리전에 따라 달라지는 값입니다. 각 리전의 AD 1을 선택합니다.|
|Size(In GB)|1024|기본값인 1TB를 사용합니다.|
|Compartment For Backup Policies|sandbox||
|Backup Policy|Bronze|이 항목의 선택은 제약이 없습니다. 다른 값 선택이 앞으로 진행에 영향이 없습니다.|
|Volume Performance|Balanced|기본값을 사용합니다. 다른 값 선택이 앞으로 진행에 영향이 없습니다.|
|Encryption|Encrypt Using Oracle-Managed Keys|기본값을 사용합니다.|

위 입력값을 모두 입력하면 다음 이미지와 같은 상태가 됩니다. 입력을 마쳤으면 __Create_Block_Volume__ 버튼을 클릭합니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/040.jpg)

### 4. Compute에 Block Volume 연결

cluster_vm01과 cluster_vm02에 앞에서 생성한 Block Volume을 연결합니다. "__shared_volume__" 상세 페이지에서 왼쪽 메뉴의 "__Attached Instances__" 링크를 클릭하고 "__Attach to Instance__" 버튼을 클릭합니다. 그리고 다음과 같은 설정을 통해서 Block Volume에 인스턴스를 추가합니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/050.jpg)

cluster_vm01과 cluster_vm02를 다음과 같은 설정으로 "__shared_volume__"에 등록합니다. 

|설정 항목|cluster_vm01 설정값|cluster_vm02 설정값|
|----|----|----|
|Attachment Type|ISCSI|ISCSI|
|Access Type|READ/WRITE SHARED|READ/WRITE SHARED|
|Settig Type|Select Instance|Select Instance|
|Choose Instance|cluster_vm01|cluster_vm02|
|Device Name|/dev/oracleoci/oraclevdb|/dev/oracleoci/oraclevdb|

위 설정으로 등록 창을 입력하면 다음과 같은 상태가 됩니다. 두 인스턴스를 지정하고 __Attach__ 버튼을 클릭합니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/060.jpg)

Block Volume에 2개 인스턴스를 등록하면 다음과 같은 상태가 됩니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/340.jpg)

Block Volume 상세 페이지에 등록된 인스턴스 목록에서 각각 __iSCSI Command & Information__ 메뉴를 실행하고 __Attach Command__를 복사합니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/330.jpg)

cluster_vm01과 cluster_vm02의 __Attach Command__는 다음과 같습니다. 

- cluster-vm01

```
sudo iscsiadm -m node -o new -T iqn.2015-12.com.oracleiaas:84a47e02-9482-4040-8476-e46468941046 -p 169.254.2.2:3260
sudo iscsiadm -m node -o update -T iqn.2015-12.com.oracleiaas:84a47e02-9482-4040-8476-e46468941046 -n node.startup -v automatic
sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:84a47e02-9482-4040-8476-e46468941046 -p 169.254.2.2:3260 -l
```

- cluster-vm02

```
sudo iscsiadm -m node -o new -T iqn.2015-12.com.oracleiaas:84a47e02-9482-4040-8476-e46468941046 -p 169.254.2.2:3260
sudo iscsiadm -m node -o update -T iqn.2015-12.com.oracleiaas:84a47e02-9482-4040-8476-e46468941046 -n node.startup -v automatic
sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:84a47e02-9482-4040-8476-e46468941046 -p 169.254.2.2:3260 -l
```

cluster_vm01과 cluster_vm02에 ssh로 접속하고 위 명령을 각각 실행합니다. 위 명령을 각각 서버에서 실행한 후, fdisk 명령을 실행하면 공유 볼륨이 연결된 것을 확인할 수 있습니다. 다음 이미지는 cluster_vm01과 cluster_vm02에서 명령을 실행하고 fdisk 명령으로 shared_volume 인식을 확인하는 과정을 보여줍니다.

![](https://taewanmerepo.github.io/2020/03/sharedvolume/090.jpg)

### 5. 2개 인스턴스에 OCFS2 설치 및 구성

cluster_vm01과 cluster_vm02에 ssh로 접속하고 다음 명령을 각각 실행하여 ocfs2-tools-devel와 ocfs2-tools 패키지를 설치합니다.

```
sudo yum install ocfs2-tools-devel ocfs2-tools -y
```

cluster_vm01과 cluster_vm02에 다음 명령을 각각 수행하여 이름이 <font color='red'>__ociocfs2__</font>인 OCIFS2 클러스터를 만듭니다. 

```
sudo o2cb add-cluster ociocfs2
```

이제 클러스터에 2개 노드를 등록합니다. 형식은 다음과 같습니다. 

```
sudo o2cb add-node <Cluster_NAME> <COMPUTE_NAME> --ip <PRIVATE_ID>
```

이 형식을 cluster_vm01과 cluster_vm02에 적용하면 다음과 같습니다. 다음 명령을 cluster_vm01과 cluster_vm02에서 각각 실행합니다. 아래 명령은 데모에서 사용하는 Private IP가 설정되어 있습니다. 이 명령를 사용하실 때 사용하시는 Compute 인스턴스의 Private IP로 수정하고 사용하시기 바랍니다.   

```
sudo o2cb add-node ociocfs2 cluster-vm01 --ip 10.0.0.27
sudo o2cb add-node ociocfs2 cluster-vm02 --ip 10.0.0.28
```

OCFS2 클러스터 설정은 __/etc/ocfs2/cluster.conf__ 파일에 저장됩니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/350.jpg)

OCFS2는 O2CB라는 클러스터 관리 컴포넌트로 5개의 시스템 서비스로 구성됩니다. 클러스터를 구성하기 위해서는 cluster_vm01과 cluster_vm02에서 O2CB를 동일하게 설정해야 합니다. O2CB 설정을 위하여 cluster_vm01과 cluster_vm02에서 각각 __sudo /sbin/o2cb.init configure__ 명령을 실행합니다. "Cluster to start on boot (Enter "none" to clear)" 항목에 __ociocfs2__를 입력하는 것을 제외하면 모두 기본값을 설정합니다.(Just Enter) 

```
[opc@cluster-vm01 ~]$ sudo /sbin/o2cb.init configure
Configuring the O2CB driver.

This will configure the on-boot properties of the O2CB driver.
The following questions will determine whether the driver is loaded on
boot.  The current values will be shown in brackets ('[]').  Hitting
<ENTER> without typing an answer will keep that current value.  Ctrl-C
will abort.

Load O2CB driver on boot (y/n) [n]: y
Cluster stack backing O2CB [o2cb]:
Cluster to start on boot (Enter "none" to clear) [ocfs2]: ociocfs2
Specify heartbeat dead threshold (>=7) [31]:
Specify network idle timeout in ms (>=5000) [30000]:
Specify network keepalive delay in ms (>=1000) [2000]:
Specify network reconnect delay in ms (>=2000) [2000]:
Writing O2CB configuration: OK
checking debugfs...
Loading stack plugin "o2cb": OK
Loading filesystem "ocfs2_dlmfs": OK
Creating directory '/dlm': OK
Mounting ocfs2_dlmfs filesystem at /dlm: OK
Setting cluster stack "o2cb": OK
Registering O2CB cluster "ociocfs2": OK
Setting O2CB cluster timeouts : OK
[opc@cluster-vm01 ~]$
```

cluster_vm01과 cluster_vm02에서 __sudo /sbin/o2cb.init status__ 명령을 실행하면 OCFS2 클러스터 스텍 상태를 확인할 수 있습니다.  

![](https://taewanmerepo.github.io/2020/03/sharedvolume/360.jpg)

cluster-vm01과 cluster-vm02 모두 클러스터 스택 중 O2CB heartbeat이 __Not Active__ 상태입니다. 


cluster_vm01과 cluster_vm02에서 __Checking O2CB cluster "ociocfs2"__ 상태가 Offline인 것을 확인할 수 있습니다. Cluster Stack을 관리하는 명령은 다음과 같습니다.

|Cluster Stack 관리 명령|설명|
|----|----|
|sudo /sbin/o2cb.init status|Cluster Stack 상태 확인|
|sudo /sbin/o2cb.init online|Cluster Stack 시작|
|sudo /sbin/o2cb.init offline|Cluster Stack 정지|
|sudo /sbin/o2cb.init unload|Cluster Stack 비활성화|


cluster_vm01과 cluster_vm02에서 다음 명령을 실행하여 o2cb와 ocfs2를 서비스에 등록합니다.

```
sudo systemctl enable o2cb
sudo systemctl enable ocfs2
```

위 명령은 다음과 같이 실행됩니다.

```
[opc@cluster-vm01 ~]$ sudo systemctl enable o2cb
Created symlink from /etc/systemd/system/multi-user.target.wants/o2cb.service to /usr/lib/systemd/system/o2cb.service.
[opc@cluster-vm01 ~]$ sudo systemctl enable ocfs2
Created symlink from /etc/systemd/system/multi-user.target.wants/ocfs2.service to /usr/lib/systemd/system/ocfs2.service.
[opc@cluster-vm01 ~]$ 
```

cluster_vm01과 cluster_vm02에서 다음 명령을 수행하여 OCFS2 클러스터를 위한 커널 설정을 합니다.

```
# Define panic and panic_on_oops for cluster operation
sudo sysctl kernel.panic=30
sudo sysctl kernel.panic_on_oops=1
```

위 명령은 다음과 같이 실행됩니다.

```
[opc@cluster-vm01 ~]$ sudo sysctl kernel.panic=30
kernel.panic = 30
[opc@cluster-vm01 ~]$ sudo sysctl kernel.panic_on_oops=1
kernel.panic_on_oops = 1
[opc@cluster-vm01 ~]$
```

cluster_vm01과 cluster_vm02에서 이 설정이 시스템 재시작  후에도 유지되도록 __/etc/sysctl.conf__파일 마지막에 다음 설정을 추가합니다.

```
# Define panic and panic_on_oops for cluster operation
kernel.panic=30
kernel.panic_on_oops=1
```

__/etc/sysctl.conf__ 파일을 수정한 결과는 다음과 같습니다.

```
[opc@cluster-vm01 ~]$ sudo vi /etc/sysctl.conf
[opc@cluster-vm01 ~]$ sudo cat /etc/sysctl.conf
# sysctl settings are defined through files in
# /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.
#
# Vendors settings live in /usr/lib/sysctl.d/.
# To override a whole file, create a new file with the same in
# /etc/sysctl.d/ and put new settings there. To override
# only specific settings, add a file with a lexically later
# name in /etc/sysctl.d/ and put new settings there.
#
# For more information, see sysctl.conf(5) and sysctl.d(5).

# Define panic and panic_on_oops for cluster operation
kernel.panic=30
kernel.panic_on_oops=1
[opc@cluster-vm01 ~]$
```

두 커널 파라미터의 의미는 다음과 같습니다. 

|파라미터 이름|설명|
|----|----|
|panic|panic이 발생한 후 지정한 시간 이후에 시스템을 리부팅, 초 단위 설정|
|panic_on_oops|1로 설정하면 Kernel OOPS가 발생  시스템 패닉으로 간주. panic에 설정된 시간 후에 리부팅|

### 6. cluster-vm01 공유 볼륨 포맷/마운트

이번 절은 cluster-vm01에만 적용하는 부분입니다. cluster-vm01에서 __sudo lsblk -f__을 실행하여 장치 파일 인식 상태를 확인할 수 있습니다. 현재 cluster-vm01에서는 block volume을 단순히 인식만 할 뿐입니다. 

```
[opc@cluster-vm01 ~]$ sudo lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
sdb
sda
├─sda2 swap         02789b15-4058-485e-86a6-38522a5c00c6 [SWAP]
├─sda3 xfs          53bb89bf-50e3-4358-9f5e-e9c5ac8d535c /
└─sda1 vfat         F831-6C7E                            /boot/efi
[opc@cluster-vm01 ~]$
```

__sudo fdisk /dev/sdb__ 명령을 실행하여 앞에서 추가한 Block Volume에 파티션을 설정합니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/380.jpg)

__sudo fdisk -l__ 명령을 실행하여 파티션 생성 결과를 확인합니다. 위 파티션 작업을 통해서 <font color='red'>__/dev/sdb1__</font>가 만들어 진 것을 확인할 수 있습니다.

```
opc@cluster-vm01 ~]$ sudo fdisk -l
WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.

Disk /dev/sda: 50.0 GB, 50010783744 bytes, 97677312 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 1048576 bytes
Disk label type: gpt
Disk identifier: 23A7AABA-6B11-4E98-A2CE-4962B4F7BCC3


#         Start          End    Size  Type            Name
 1         2048       411647    200M  EFI System      EFI System Partition
 2       411648     17188863      8G  Linux swap
 3     17188864     97675263   38.4G  Microsoft basic

Disk /dev/sdb: 1099.5 GB, 1099511627776 bytes, 2147483648 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 1048576 bytes
Disk label type: dos
Disk identifier: 0xcf42a827

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1            2048   104859647    52428800   83  Linux
[opc@cluster-vm01 ~]$ sudo ls -al /dev/sd*
brw-rw----. 1 root disk 8,  0  3월 10 08:11 /dev/sda
brw-rw----. 1 root disk 8,  1  3월 10 08:11 /dev/sda1
brw-rw----. 1 root disk 8,  2  3월 10 08:11 /dev/sda2
brw-rw----. 1 root disk 8,  3  3월 10 08:11 /dev/sda3
brw-rw----. 1 root disk 8, 16  3월 10 09:28 /dev/sdb
brw-rw----. 1 root disk 8, 17  3월 10 09:28 /dev/sdb1
[opc@cluster-vm01 ~]$
```

다음 명령을 이용하여 /dev/sdb1 파티션을 ocfs2 타입으로 포맷합니다. 

```
sudo mkfs.ocfs2 -L "ocfs2" /dev/sdb1
```

위 명령을 실행하면 다음과 같은 결과가 출력됩니다. 

```
[opc@cluster-vm01 ~]$ sudo mkfs.ocfs2 -L "ocfs2" /dev/sdb1
mkfs.ocfs2 1.8.6
Cluster stack: classic o2cb
Label: ocfs2
Features: sparse extended-slotmap backup-super unwritten inline-data strict-journal-super xattr indexed-dirs refcount discontig-bg
Block size: 4096 (12 bits)
Cluster size: 4096 (12 bits)
Volume size: 53687091200 (13107200 clusters) (13107200 blocks)
Cluster groups: 407 (tail covers 11264 clusters, rest cover 32256 clusters)
Extent allocator size: 8388608 (2 groups)
Journal size: 268435456
Node slots: 8
Creating bitmaps: done
Initializing superblock: done
Writing system files: done
Writing superblock: done
Writing backup superblock: 3 block(s)
Formatting Journals: done
Growing extent allocator: done
Formatting slot map: done
Formatting quota files: done
Writing lost+found: done
mkfs.ocfs2 successful

[opc@cluster-vm01 ~]$
```

__sudo lsblk -f__ 명령으로 /dev/sdb1의 포맷 결과를 확인할 수 있습니다. /dev/sdb1는 ocfs2 포맷으로 포맷되었고, 레이블 명은 "ocfs2"입니다. 

```
[opc@cluster-vm01 ~]$ sudo lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
sdb
└─sdb1 ocfs2  ocfs2 d40356de-ed1a-47a0-9866-86906bca7cba
sda
├─sda2 swap         02789b15-4058-485e-86a6-38522a5c00c6 [SWAP]
├─sda3 xfs          53bb89bf-50e3-4358-9f5e-e9c5ac8d535c /
└─sda1 vfat         F831-6C7E                            /boot/efi
[opc@cluster-vm01 ~]$
```

다음 명령으로 /dev/sdb1를 마운트할 디렉터리를 만듭니다. 

```
sudo mkdir /u01
```

그리고 __sudo vi /etc/fstab__ 명령을 실행하고 다음 마운트 설정을 추가합니다. 

```
/dev/sdb1 /u01 ocfs2     _netdev,defaults   0 0 
```

![](https://taewanmerepo.github.io/2020/03/sharedvolume/120.jpg)

다음 명령으로 __etc/fstab__에 파일 시스템 정보를 마운트합니다.

```
sudo mount -a
```

파일 시스템을 마운트하고 lsblk 명령으로 리눅스 볼륨 구성을 확인할 수 있습니다.

```
[opc@cluster-vm01 ~]$ sudo lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
sdb
└─sdb1 ocfs2  ocfs2 d40356de-ed1a-47a0-9866-86906bca7cba /u01
sda
├─sda2 swap         02789b15-4058-485e-86a6-38522a5c00c6 [SWAP]
├─sda3 xfs          53bb89bf-50e3-4358-9f5e-e9c5ac8d535c /
└─sda1 vfat         F831-6C7E                            /boot/efi
[opc@cluster-vm01 ~]$
```

__/dev/sdb1__이 __/u01__에 마운트된 것을 확인할 수 있습니다. cluster_vm01에서 "__sudo /sbin/o2cb.init status__" 명령을 실행하면 이제 OCFS2 클러스터 스택 상태를 확인할 수 있습니다. 클러스터 스택의 O2CB heartbeat이  "__Active__" 상태가 된 것을 확인할 수 있습니다. 

![](https://taewanmerepo.github.io/2020/03/sharedvolume/390.jpg)

### 7. cluster-vm01 공유 볼륨 테스트

다음과 같이 cluster-vm01에서 마운트한 공유 블록 스토리지에 파일을 쓰고 조회할 수 있습니다. 

```
[opc@cluster-vm01 ~]$ sudo cp /boot/vmlinuz* /u01
[opc@cluster-vm01 ~]$ sudo ls -al /u01
합계 28664
drwxr-xr-x.  3 root root    3896  3월 10 10:10 .
dr-xr-xr-x. 19 root root    4096  3월 10 09:53 ..
drwxr-xr-x.  2 root root    3896  3월 10 09:47 lost+found
-rwxr-xr-x.  1 root root 7534112  3월 10 10:10 vmlinuz-0-rescue-185a1faed8734f9d8fd4c288da75d2e4
-rwxr-xr-x.  1 root root 7532256  3월 10 10:10 vmlinuz-0-rescue-33f2b4bac8b045d281f1e24789d4cb5b
-rwxr-xr-x.  1 root root 6735520  3월 10 10:10 vmlinuz-3.10.0-1062.12.1.el7.x86_64
-rwxr-xr-x.  1 root root 7532256  3월 10 10:10 vmlinuz-4.14.35-1902.10.8.el7uek.x86_64
[opc@cluster-vm01 ~]$
```

### 8. cluster-vm02 공유 볼륨 마운트

lsblk 명령으로 리눅스 볼륨 구성을 확인할 수 있습니다. 

```
[opc@cluster-vm02 ~]$ sudo lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
sdb
sda
├─sda2 swap         02789b15-4058-485e-86a6-38522a5c00c6 [SWAP]
├─sda3 xfs          53bb89bf-50e3-4358-9f5e-e9c5ac8d535c /
└─sda1 vfat         F831-6C7E                            /boot/efi
[opc@cluster-vm02 ~]$
```

위 결과를 보면 cluster-vm02는 /dev/sdb의 파일 포맷과 레이블을 인식하지 못하고 있습니다. cluster-vm01에서 공유 볼륨을 파티션을 만들고 포맷헷지만, 이 정보를 cluster-vm02 커널은 아직 인식하지 못하는 상태입니다. 

현재 사용 중인 HDD에 특정 파티션을 추가하거나 변형했을 때, 리눅스 커널은 사용 중인 디바이스 변경을 인식하지 못합니다. partprobe 명령은 운영 중인 파티션 정보를 커널에 업데이트 역할을 담당합니다. 다음과 같은 형식으로 사용됩니다.  

```
sudo partprobe /디바이스명
```
 
"__partprobe__"를 실행한 다음에 "__sudo lsblk -f__"를 실행하면 /dev/sdb1의 파일 시스템 타입과 레이블을 cluster-vm02 커널이 인식하는 것을 확인할 수 있습니다.  

```
[opc@cluster-vm02 ~]$ sudo partprobe /dev/sdb
[opc@cluster-vm02 ~]$ sudo lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
sdb
└─sdb1 ocfs2  ocfs2 d40356de-ed1a-47a0-9866-86906bca7cba
sda
├─sda2 swap         02789b15-4058-485e-86a6-38522a5c00c6 [SWAP]
├─sda3 xfs          53bb89bf-50e3-4358-9f5e-e9c5ac8d535c /
└─sda1 vfat         F831-6C7E                            /boot/efi
[opc@cluster-vm02 ~]$
```

cluster-vm02에서 다음 명령으로 /dev/sdb1를 마운트할 디렉터리를 만듭니다. 

```
sudo mkdir /u01
```

그리고 __sudo vi /etc/fstab__ 명령을 실행하고 다음 마운트 설정을 추가합니다. 

```
/dev/sdb1 /u01 ocfs2     _netdev,defaults   0 0 
```

![](https://taewanmerepo.github.io/2020/03/sharedvolume/400.jpg)

다음 명령으로 __etc/fstab__의 파일 시스템 정보를 마운트합니다.

```
sudo mount -a
```

파일 시스템을 마운트하고 lsblk 명령으로 리눅스 볼륨 구성을 확인할 수 있습니다.

```
[[opc@cluster-vm02 ~]$ sudo mount -a
[opc@cluster-vm02 ~]$ sudo lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
sdb
└─sdb1 ocfs2  ocfs2 d40356de-ed1a-47a0-9866-86906bca7cba /u01
sda
├─sda2 swap         02789b15-4058-485e-86a6-38522a5c00c6 [SWAP]
├─sda3 xfs          53bb89bf-50e3-4358-9f5e-e9c5ac8d535c /
└─sda1 vfat         F831-6C7E                            /boot/efi
[opc@cluster-vm02 ~]$
```

### 9. cluster-vm02 공유 볼륨 테스트

cluster-vm02에서 공유 볼륨인 /u01에 데이터를 쓰고 조회할 수 있습니다. 

```
[opc@cluster-vm02 ~]$ sudo mkdir /u01/demo
[opc@cluster-vm02 ~]$ sudo cp /boot/vmlinuz* /u01/demo/
[opc@cluster-vm02 ~]$ sudo ls -al /u01/demo/
합계 28660
drwxr-xr-x. 2 root root    3896  3월 10 11:05 .
drwxr-xr-x. 4 root root    3896  3월 10 11:05 ..
-rwxr-xr-x. 1 root root 7534112  3월 10 11:05 vmlinuz-0-rescue-185a1faed8734f9d8fd4c288da75d2e4
-rwxr-xr-x. 1 root root 7532256  3월 10 11:05 vmlinuz-0-rescue-33f2b4bac8b045d281f1e24789d4cb5b
-rwxr-xr-x. 1 root root 6735520  3월 10 11:05 vmlinuz-3.10.0-1062.12.1.el7.x86_64
-rwxr-xr-x. 1 root root 7532256  3월 10 11:05 vmlinuz-4.14.35-1902.10.8.el7uek.x86_64
[opc@cluster-vm02 ~]$
```

## 요약

OCI(Oracle Cloud Infrastructure)는 여러 Compute 인스턴스에 공유 스토리지를 구성하는 방법을 제공합니다. Block Storage를 여러 Compute 인스턴스에 연결하고 마운트하는 방식입니다. 

여러 Compute 인스턴스가 특정 스토리지를 공유할 때, 안전한 데이터 관리를 위해서 I/O를 조율(Coodination)하는 Clusterwae가 필요합니다. OCI는 현재 OCSF2(Oracle Cluster File System Version 2), GlusterFS, IBM Spectrum Scale을 지원합니다. 

이번 문서에서는 OCFS2를 이용하여 2개 Compute 인스턴스가 Block Volume을 공유하는 방법을 정리했습니다.  

## 참고자료
- {{< extlink link="https://blogs.oracle.com/cloud-infrastructure/using-the-multi-attach-block-volume-feature-to-create-a-shared-file-system-on-oracle-cloud-infrastructure" desc="[Oracle Blog]Using the Multiple-Instance Attach Block Volume Feature to Create a Shared File System on Oracle Cloud Infrastructure">}}
- {{< extlink link="http://www.iorchard.net/docs/pengx2/pengx2_ocfs2.html" desc="[한글블로그]OCFS2 Cluster 구성">}}
- {{< extlink link="https://www.youtube.com/watch?v=Mw5LzHVX7AU " desc="[Youtube]Configure and Test an OCFS2 Cluster with Oracle Linux on Oracle Cloud Infrastructure">}}








